{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO8rwcrpUyZ3FOoyXyulbXl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Immanuel-01/Global-Health/blob/main/Copy_of_Global_Health.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxXeNxs2ZTWF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "gbh = pd.read_csv(\"Global Health Statistics.csv\")\n",
        "\n",
        "# Check the number of rows in the dataset\n",
        "print(f\"Total rows in the dataset: {len(gbh)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of rows in the subset\n",
        "print(f\"Number of rows in the subset: {len(gbh)}\")"
      ],
      "metadata": {
        "id": "aD_cyG_Yec0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the dataset\n",
        "\n",
        "print(gbh.head())"
      ],
      "metadata": {
        "id": "iCesPiaEadq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gbh.info())   # Get basic information about the dataset"
      ],
      "metadata": {
        "id": "69RGgUsjavHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gbh.describe())  # Summary statistics for numerical columns"
      ],
      "metadata": {
        "id": "ImXK2bWna04_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "\n",
        "print(gbh.isnull().sum())"
      ],
      "metadata": {
        "id": "MSd-Eo27bA8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Impute missing values for numerical columns with the median\n",
        "numerical_columns = ['Average Treatment Cost (USD)', 'Recovery Rate (%)', 'DALYs',\n",
        "                     'Improvement in 5 Years (%)', 'Per Capita Income (USD)',\n",
        "                     'Education Index', 'Urbanization Rate (%)']\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "gbh[numerical_columns] = imputer.fit_transform(gbh[numerical_columns])\n"
      ],
      "metadata": {
        "id": "aHgI6YTEmrU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values for categorical columns with the most frequent category\n",
        "categorical_columns = ['Treatment Type', 'Availability of Vaccines/Treatment']\n",
        "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "gbh[categorical_columns] = imputer_cat.fit_transform(gbh[categorical_columns])\n"
      ],
      "metadata": {
        "id": "Ja0-AJOrmtt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gbh.isnull().sum())  # Check if there are still any missing values\n"
      ],
      "metadata": {
        "id": "FFsOvR1smxaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicates\n",
        "print(gbh.duplicated().sum())"
      ],
      "metadata": {
        "id": "AjBlNYslf390"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "f9rqxnA_f-Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diseases_counts = gbh['Disease Name'].value_counts()  # Count the occurrences of each disease\n",
        "\n",
        "# Plot the distribution of diseases\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=diseases_counts.index, y=diseases_counts.values, palette=\"viridis\")\n",
        "plt.title(\"Distribution of Diseases\")\n",
        "plt.xlabel(\"Disease\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ng3ws4SegFQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by country and disease\n",
        "\n",
        "country_diseases = gbh.groupby(['Disease Name','Country']).size().unstack()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(country_diseases, cmap=\"YlGnBu\", annot=True, fmt=\"g\")\n",
        "plt.title(\"Disease Prevalence by Country\")\n",
        "plt.xlabel(\"Disease\")\n",
        "plt.ylabel(\"Country\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6mqerreYgnOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the percentage of missing data in each column\n",
        "missing_percentage = gbh.isnull().mean() * 100\n",
        "print(missing_percentage)\n"
      ],
      "metadata": {
        "id": "Fx0gM-dHDQcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform one-hot encoding for categorical columns\n",
        "gbh = pd.get_dummies(gbh, columns=['Country', 'Disease Name', 'Treatment Type'], drop_first=True)\n"
      ],
      "metadata": {
        "id": "GVbJZMU9DVa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# List of numerical columns for scaling\n",
        "numerical_columns = ['Prevalence Rate (%)', 'Incidence Rate (%)', 'Mortality Rate (%)',\n",
        "                     'Population Affected', 'Healthcare Access (%)', 'Doctors per 1000',\n",
        "                     'Hospital Beds per 1000', 'Average Treatment Cost (USD)', 'Recovery Rate (%)',\n",
        "                     'DALYs', 'Improvement in 5 Years (%)', 'Per Capita Income (USD)', 'Education Index',\n",
        "                     'Urbanization Rate (%)']\n",
        "\n",
        "# Scale the numerical features\n",
        "scaler = StandardScaler()\n",
        "gbh[numerical_columns] = scaler.fit_transform(gbh[numerical_columns])\n"
      ],
      "metadata": {
        "id": "8DHkp2AyDZjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering: Combine healthcare access and doctors per 1000\n",
        "gbh['Healthcare_Quality'] = gbh['Healthcare Access (%)'] * gbh['Doctors per 1000']\n"
      ],
      "metadata": {
        "id": "EIlc3I4JDeta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a binary classification target based on Prevalence Rate (%)\n",
        "# Use median as the threshold to define high/low prevalence\n",
        "threshold = gbh['Prevalence Rate (%)'].median()\n",
        "gbh['Prevalence Category'] = (gbh['Prevalence Rate (%)'] > threshold).astype(int)\n",
        "\n",
        "# Check the new target column\n",
        "print(gbh[['Prevalence Rate (%)', 'Prevalence Category']].head())\n"
      ],
      "metadata": {
        "id": "B6UTk9noF4pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Include the newly engineered 'Healthcare_Quality' feature\n",
        "X = gbh.drop(columns=['Prevalence Rate (%)', 'Prevalence Category'])  # Exclude target and original prevalence\n",
        "y = gbh['Prevalence Category']  # Target variable for classification (high/low prevalence)\n"
      ],
      "metadata": {
        "id": "L6F5Ju0LE9Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Include the newly engineered 'Healthcare_Quality' feature\n",
        "X = gbh.drop(columns=['Prevalence Rate (%)', 'Prevalence Category'])  # Exclude target and original prevalence\n",
        "y = gbh['Prevalence Category']  # Target variable for classification (high/low prevalence)\n",
        "\n",
        "# Verify the shape of X and y\n",
        "print(X.shape, y.shape)\n"
      ],
      "metadata": {
        "id": "G560TncGF-qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the split\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Test data shape: {X_test.shape}\")\n"
      ],
      "metadata": {
        "id": "yV8PVdpsGKPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot Encode the categorical variables\n",
        "X_encoded = pd.get_dummies(X, drop_first=True)  # drop_first to avoid multicollinearity\n",
        "\n",
        "# Define the target variable\n",
        "y = gbh['Prevalence Category']  # This should already be binary (0 and 1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Logistic Regression model\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "print(\"Logistic Regression Evaluation\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr)}\")\n",
        "print(f\"Confusion Matrix: \\n{confusion_matrix(y_test, y_pred_lr)}\")\n",
        "print(f\"Classification Report: \\n{classification_report(y_test, y_pred_lr)}\")\n"
      ],
      "metadata": {
        "id": "SDTuibV5GOz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize the feature set\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "7B7btslUg7jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100], 'solver': ['liblinear', 'lbfgs']}\n",
        "grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Use the best model\n",
        "best_lr_model = grid_search.best_estimator_\n",
        "y_pred_best_lr = best_lr_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the tuned Logistic Regression model\n",
        "print(\"Improved Logistic Regression Evaluation\")\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_best_lr)}\")\n",
        "print(f\"Confusion Matrix: \\n{confusion_matrix(y_test, y_pred_best_lr)}\")\n",
        "print(f\"Classification Report: \\n{classification_report(y_test, y_pred_best_lr)}\")\n"
      ],
      "metadata": {
        "id": "Z4GDHrMeg2Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "print(\"Random Forest Classifier Evaluation\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf)}\")\n",
        "print(f\"Confusion Matrix: \\n{confusion_matrix(y_test, y_pred_rf)}\")\n",
        "print(f\"Classification Report: \\n{classification_report(y_test, y_pred_rf)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "D6tQfWpghFYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost Classifier\n",
        "import xgboost as xgb\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "print(\"XGBoost Classifier Evaluation\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb)}\")\n",
        "print(f\"Confusion Matrix: \\n{confusion_matrix(y_test, y_pred_xgb)}\")\n",
        "print(f\"Classification Report: \\n{classification_report(y_test, y_pred_xgb)}\")\n"
      ],
      "metadata": {
        "id": "Sx6qvNoWhLt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance from Random Forest\n",
        "feature_importance_rf = rf_model.feature_importances_\n",
        "important_features = pd.Series(feature_importance_rf, index=X_encoded.columns).sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "important_features.head(10).plot(kind='bar', color='skyblue')\n",
        "plt.title(\"Top 10 Feature Importances (Random Forest)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "w-NMJS_UhW2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode target labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train_encoded, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_scaled, y_test_encoded)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "ixWFx01Chlii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(\"Unique Classes in y_train_encoded:\", np.unique(y_train_encoded))\n",
        "\n"
      ],
      "metadata": {
        "id": "EyOHCGhSic-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Original target distribution\n",
        "print(\"Original Class Distribution in y_train_encoded:\")\n",
        "print(pd.Series(y_train_encoded).value_counts())\n"
      ],
      "metadata": {
        "id": "jXliQ3A-iu3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Model architecture with hyperparameters\n",
        "def build_model(learning_rate=0.001, dropout_rate=0.3):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')  # Binary classification\n",
        "    ])\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "QDEWJSyfjRZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with default parameters\n",
        "model = build_model()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train_encoded,\n",
        "                     validation_data=(X_test_scaled, y_test_encoded),\n",
        "                     epochs=50,\n",
        "                     batch_size=32,\n",
        "                     verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMLIKhgajd2D",
        "outputId": "9c7532f2-1820-4f2e-a234-abd8b20afb01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2ms/step - accuracy: 0.5016 - loss: 0.6986 - val_accuracy: 0.4977 - val_loss: 0.6933\n",
            "Epoch 2/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2ms/step - accuracy: 0.4998 - loss: 0.6933 - val_accuracy: 0.5023 - val_loss: 0.6932\n",
            "Epoch 3/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2ms/step - accuracy: 0.5017 - loss: 0.6932 - val_accuracy: 0.5023 - val_loss: 0.6932\n",
            "Epoch 4/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2ms/step - accuracy: 0.5001 - loss: 0.6932 - val_accuracy: 0.4981 - val_loss: 0.6932\n",
            "Epoch 5/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - accuracy: 0.5012 - loss: 0.6932 - val_accuracy: 0.5022 - val_loss: 0.6931\n",
            "Epoch 6/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2ms/step - accuracy: 0.5022 - loss: 0.6931 - val_accuracy: 0.4979 - val_loss: 0.6931\n",
            "Epoch 7/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2ms/step - accuracy: 0.5000 - loss: 0.6932 - val_accuracy: 0.5021 - val_loss: 0.6932\n",
            "Epoch 8/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2ms/step - accuracy: 0.5007 - loss: 0.6931 - val_accuracy: 0.5023 - val_loss: 0.6932\n",
            "Epoch 9/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - accuracy: 0.5018 - loss: 0.6931 - val_accuracy: 0.4972 - val_loss: 0.6933\n",
            "Epoch 10/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2ms/step - accuracy: 0.5041 - loss: 0.6930 - val_accuracy: 0.4972 - val_loss: 0.6933\n",
            "Epoch 11/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2ms/step - accuracy: 0.5053 - loss: 0.6931 - val_accuracy: 0.4975 - val_loss: 0.6932\n",
            "Epoch 12/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - accuracy: 0.5015 - loss: 0.6930 - val_accuracy: 0.5026 - val_loss: 0.6932\n",
            "Epoch 13/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2ms/step - accuracy: 0.5033 - loss: 0.6928 - val_accuracy: 0.5024 - val_loss: 0.6933\n",
            "Epoch 14/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - accuracy: 0.5045 - loss: 0.6929 - val_accuracy: 0.5022 - val_loss: 0.6934\n",
            "Epoch 15/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2ms/step - accuracy: 0.5040 - loss: 0.6929 - val_accuracy: 0.4973 - val_loss: 0.6933\n",
            "Epoch 16/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - accuracy: 0.5032 - loss: 0.6929 - val_accuracy: 0.4974 - val_loss: 0.6933\n",
            "Epoch 17/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - accuracy: 0.5052 - loss: 0.6927 - val_accuracy: 0.5025 - val_loss: 0.6939\n",
            "Epoch 18/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - accuracy: 0.5057 - loss: 0.6926 - val_accuracy: 0.4972 - val_loss: 0.6936\n",
            "Epoch 19/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - accuracy: 0.5041 - loss: 0.6927 - val_accuracy: 0.4970 - val_loss: 0.6935\n",
            "Epoch 20/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - accuracy: 0.5045 - loss: 0.6927 - val_accuracy: 0.4966 - val_loss: 0.6937\n",
            "Epoch 21/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2ms/step - accuracy: 0.5071 - loss: 0.6926 - val_accuracy: 0.5026 - val_loss: 0.6935\n",
            "Epoch 22/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - accuracy: 0.5057 - loss: 0.6925 - val_accuracy: 0.4976 - val_loss: 0.6935\n",
            "Epoch 23/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - accuracy: 0.5053 - loss: 0.6925 - val_accuracy: 0.4976 - val_loss: 0.6935\n",
            "Epoch 24/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - accuracy: 0.5057 - loss: 0.6924 - val_accuracy: 0.4980 - val_loss: 0.6937\n",
            "Epoch 25/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2ms/step - accuracy: 0.5068 - loss: 0.6921 - val_accuracy: 0.4978 - val_loss: 0.6934\n",
            "Epoch 26/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2ms/step - accuracy: 0.5069 - loss: 0.6922 - val_accuracy: 0.4982 - val_loss: 0.6935\n",
            "Epoch 27/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2ms/step - accuracy: 0.5056 - loss: 0.6923 - val_accuracy: 0.4977 - val_loss: 0.6933\n",
            "Epoch 28/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2ms/step - accuracy: 0.5075 - loss: 0.6921 - val_accuracy: 0.4977 - val_loss: 0.6934\n",
            "Epoch 29/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2ms/step - accuracy: 0.5065 - loss: 0.6921 - val_accuracy: 0.4980 - val_loss: 0.6938\n",
            "Epoch 30/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2ms/step - accuracy: 0.5075 - loss: 0.6922 - val_accuracy: 0.4979 - val_loss: 0.6934\n",
            "Epoch 31/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2ms/step - accuracy: 0.5078 - loss: 0.6921 - val_accuracy: 0.4976 - val_loss: 0.6935\n",
            "Epoch 32/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2ms/step - accuracy: 0.5077 - loss: 0.6920 - val_accuracy: 0.5029 - val_loss: 0.6934\n",
            "Epoch 33/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2ms/step - accuracy: 0.5087 - loss: 0.6918 - val_accuracy: 0.4977 - val_loss: 0.6934\n",
            "Epoch 34/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2ms/step - accuracy: 0.5080 - loss: 0.6917 - val_accuracy: 0.4982 - val_loss: 0.6936\n",
            "Epoch 35/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2ms/step - accuracy: 0.5061 - loss: 0.6919 - val_accuracy: 0.4976 - val_loss: 0.6935\n",
            "Epoch 36/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2ms/step - accuracy: 0.5052 - loss: 0.6918 - val_accuracy: 0.4979 - val_loss: 0.6934\n",
            "Epoch 37/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2ms/step - accuracy: 0.5087 - loss: 0.6919 - val_accuracy: 0.5029 - val_loss: 0.6933\n",
            "Epoch 38/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2ms/step - accuracy: 0.5071 - loss: 0.6917 - val_accuracy: 0.4974 - val_loss: 0.6935\n",
            "Epoch 39/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2ms/step - accuracy: 0.5071 - loss: 0.6917 - val_accuracy: 0.4978 - val_loss: 0.6934\n",
            "Epoch 40/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - accuracy: 0.5080 - loss: 0.6916 - val_accuracy: 0.4974 - val_loss: 0.6936\n",
            "Epoch 41/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - accuracy: 0.5080 - loss: 0.6918 - val_accuracy: 0.4975 - val_loss: 0.6936\n",
            "Epoch 42/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - accuracy: 0.5057 - loss: 0.6917 - val_accuracy: 0.4973 - val_loss: 0.6940\n",
            "Epoch 43/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2ms/step - accuracy: 0.5065 - loss: 0.6916 - val_accuracy: 0.4979 - val_loss: 0.6937\n",
            "Epoch 44/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2ms/step - accuracy: 0.5063 - loss: 0.6917 - val_accuracy: 0.4975 - val_loss: 0.6937\n",
            "Epoch 45/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2ms/step - accuracy: 0.5063 - loss: 0.6915 - val_accuracy: 0.4980 - val_loss: 0.6936\n",
            "Epoch 46/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2ms/step - accuracy: 0.5076 - loss: 0.6916 - val_accuracy: 0.4973 - val_loss: 0.6936\n",
            "Epoch 47/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2ms/step - accuracy: 0.5070 - loss: 0.6914 - val_accuracy: 0.4974 - val_loss: 0.6939\n",
            "Epoch 48/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2ms/step - accuracy: 0.5085 - loss: 0.6915 - val_accuracy: 0.4974 - val_loss: 0.6938\n",
            "Epoch 49/50\n",
            "\u001b[1m12874/12874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2ms/step - accuracy: 0.5066 - loss: 0.6914 - val_accuracy: 0.5031 - val_loss: 0.6935\n",
            "Epoch 50/50\n",
            "\u001b[1m 5955/12874\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5074 - loss: 0.6913"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KuubtZfzjkAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test_encoded, verbose=0)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "kz3e93d6joY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test set\n",
        "y_pred_probs = model.predict(X_test_scaled)\n",
        "y_pred_classes = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_encoded, y_pred_classes))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_encoded, y_pred_classes))\n"
      ],
      "metadata": {
        "id": "Jbe0-0HJjqzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = pd.DataFrame({\n",
        "    'Country': ['United Kingdom'],\n",
        "    'Year': [2025],\n",
        "    'Disease Name': ['Disease_ABC'],  # This would be mapped/encoded if necessary\n",
        "    'Healthcare Access (%)': [92],  # Example value for UK\n",
        "    'Doctors per 1000': [2.8],  # Example value for UK\n",
        "    'Population Affected': [200000],  # Example affected population\n",
        "    'Average Treatment Cost (USD)': [1500],  # Example value\n",
        "    'Per Capita Income (USD)': [45000],  # Example value for UK\n",
        "    'Urbanization Rate (%)': [80],  # Example urbanization rate\n",
        "    'Recovery Rate (%)': [85],  # Example recovery rate\n",
        "    'DALYs': [4.5]  # Example value\n",
        "})\n"
      ],
      "metadata": {
        "id": "0CeJ0iCGosHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the new data (same as you did for training)\n",
        "new_data_scaled = scaler.transform(new_data.drop(columns=['Country', 'Disease Name']))  # Exclude categorical columns\n",
        "\n",
        "# If needed, you can also encode categorical variables, for example:\n",
        "# new_data_encoded = encoder.transform(new_data['Disease Name'])\n"
      ],
      "metadata": {
        "id": "oVCThevmpCPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trained deep learning model to make predictions\n",
        "prediction = model.predict(new_data_scaled)\n",
        "\n",
        "# Since it's binary classification (0 or 1), you may want to interpret the result:\n",
        "prediction_label = 'High Prevalence' if prediction[0] > 0.5 else 'Low Prevalence'\n",
        "print(f\"Predicted Prevalence for Disease_ABC in the UK: {prediction_label}\")\n"
      ],
      "metadata": {
        "id": "cQabkIyKpE2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_probability = model.predict_proba(new_data_scaled)\n",
        "print(f\"Predicted Probability for High Prevalence: {predicted_probability[0][1]}\")\n"
      ],
      "metadata": {
        "id": "l_hSSAmipHoJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}